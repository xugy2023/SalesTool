import os
from itertools import groupby

from faster_whisper import WhisperModel
from pyannote.audio import Pipeline
from pydub import AudioSegment
import tempfile
import torch
import torchaudio
import wave
import contextlib
from huggingface_hub import login

# è¯»å– HuggingFace tokenï¼ˆä»ç¯å¢ƒå˜é‡ä¸­ï¼‰
token = os.getenv("HUGGINGFACE_TOKEN")
if not token:
    raise EnvironmentError("âŒ è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ HUGGINGFACE_TOKEN")
else:
    print("ğŸ” HuggingFace token loaded.")

# ç™»å½• HuggingFace
login(token)

# åˆå§‹åŒ– Whisper æ¨¡å‹ï¼ˆæœ¬åœ°è¯­éŸ³è½¬æ–‡å­—ï¼‰
model = WhisperModel("small", compute_type="int8")

# åˆå§‹åŒ– PyAnnote è¯´è¯äººåˆ†ç¦»æ¨¡å‹
diarization_pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization",
    use_auth_token=token
)

# è·å–éŸ³é¢‘æ—¶é•¿
def get_audio_duration(filepath):
    try:
        # è‡ªåŠ¨æ ¹æ®æ ¼å¼è¯»å–éŸ³é¢‘ï¼ˆæ”¯æŒmp3ã€wavã€flacã€m4aç­‰ï¼‰
        audio = AudioSegment.from_file(filepath)
        duration = len(audio) / 1000.0  # è½¬æ¢ä¸ºç§’
        return duration
    except Exception as e:
        print("âŒ æ— æ³•è¯»å–éŸ³é¢‘æ—¶é•¿ï¼š", e)
        return 0

# å°†ä»»ä½•æ ¼å¼çš„éŸ³é¢‘ç»Ÿä¸€è½¬æˆ wav ä¸´æ—¶æ–‡ä»¶
def convert_to_wav(filepath):
    if filepath.endswith(".wav"):
        return filepath  # å·²æ˜¯ wavï¼Œä¸å¤„ç†

    try:
        audio = AudioSegment.from_file(filepath)
        new_path = filepath.rsplit(".", 1)[0] + ".wav"
        audio.export(new_path, format="wav")
        return new_path
    except Exception as e:
        print(f"[é”™è¯¯] éŸ³é¢‘è½¬æ¢å¤±è´¥: {e}")
        return None




# æ‰§è¡Œè¯´è¯äººåˆ†ç¦» + è¯­éŸ³è¯†åˆ«
def transcribe_with_speakers(filepath):
    try:
        filepath = convert_to_wav(filepath)
        if not filepath:
            print("âŒ éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥")
            return None

        # è·å–éŸ³é¢‘æ—¶é•¿
        duration = get_audio_duration(filepath)
        if duration < 1:
            print("âŒ éŸ³é¢‘è¿‡çŸ­æˆ–æ— æ•ˆ")
            return None

        # æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
        diarization = diarization_pipeline(filepath)
        print("âœ… å®Œæˆè¯´è¯äººåˆ†ç¦»")

        # å°†è¯´è¯äººåˆ†ç¦»ç»“æœæ•´ç†ä¸º [(start, end, speaker)] åˆ—è¡¨
        speaker_segments = []
        for turn in diarization.itertracks(yield_label=True):
            start = turn[0].start
            end = turn[0].end
            speaker = turn[2]
            speaker_segments.append((start, end, speaker))

        # æ‰§è¡Œè¯­éŸ³è¯†åˆ«
        segments, _ = model.transcribe(filepath, beam_size=5, language="zh")
        print("âœ… å®Œæˆè¯­éŸ³è¯†åˆ«")

        # å°†è¯†åˆ«ç»“æœæ˜ å°„åˆ°å¯¹åº”çš„è¯´è¯äºº
        dialogue = []
        for segment in segments:
            segment_start = segment.start
            segment_end = segment.end
            text = segment.text.strip()
            # é»˜è®¤æœªè¯†åˆ«åˆ°è¯´è¯äºº
            speaker_label = speaker if speaker else "SPEAKER"
            # åœ¨è¯´è¯äººåˆ—è¡¨ä¸­æŸ¥æ‰¾å½“å‰è¯†åˆ«ç‰‡æ®µå±äºå“ªä¸ªäººè¯´çš„
            for start, end, speaker in speaker_segments:
                # å¦‚æœæ—¶é—´æ®µæœ‰äº¤é›†ï¼ˆå…è®¸ä¸€å®šé‡å ï¼‰
                if start <= segment_start <= end or start <= segment_end <= end:
                    speaker_label = speaker
                    break
            dialogue.append((speaker_label, text))

        if not dialogue:
            print("âš  æ²¡æœ‰ä»»ä½•è¯†åˆ«æ–‡æœ¬")
            return None

        # æŒ‰ç…§è¯´è¯äººè¿ç»­è¯´çš„å†…å®¹è¿›è¡Œåˆ†æ®µç»„åˆ
        grouped = []
        for speaker, lines in groupby(dialogue, key=lambda x: x[0]):
            lines_text = " ".join(line for _, line in lines)
            grouped.append(f"{speaker}ï¼š{lines_text}")

        result_text =  "\n\n".join(grouped)
        print("âœ… è¯­éŸ³è½¬å†™è¾“å‡ºï¼š", result_text)
        return result_text

    except Exception as e:
        print(f"[é”™è¯¯] è½¬å†™å¤±è´¥ï¼š{e}")
        return None

